{"./":{"url":"./","title":"Introduction","keywords":"","body":" 毕业设计 机器人操作系统ROS对Dobot Magicial机械臂的简单控制 这里只是一些简单的操作，并没有太多的理论 "},"dobot在rviz中显示.html":{"url":"dobot在rviz中显示.html","title":"第一步","keywords":"","body":"Dobot在Rviz中显示 为了在Rviz中展示Dobot机械臂，我们需要按照以下步骤操作： 首先，您需要创建一个新的ROS工作空间，命名为dobot_ws，随后在这个工作空间中创建一个名为dobot的功能包。 然后，创建必要的文件夹，包括launch、urdf、meshes等。这里直接导入官方提供的功能包即可。 最后，在launch文件夹中创建一个名为dobot.launch的launch文件，内容如下： 启动命令 roslaunch dobot dobot.launch 此时你可以在Rviz中看到Dobot机械臂的模型了,并可以通过插件控制各关节移动 "},"利用Moveit!生成相应配置文件.html":{"url":"利用Moveit!生成相应配置文件.html","title":"第二步","keywords":"","body":"Moveit!生成相应配置文件 启动命令 rosrun moveit_setup_assistant moveit_setup_assistant.py 1.1 创建新的Moveit!配置文件,并加载URDF文件，点击load files 1.2 自碰撞检测设置，按照默认值就ok。 什么是自碰撞检测配置? 机器人在规划运动轨迹的时候可能会出现自身结构相互碰撞的情况，所以需要设置自碰撞检测。 1.3 设置虚拟关节 什么是虚拟关节? 虚拟关节就是定义一个关节将机器人与世界链接起来，这里并没有进行设置 1.4 创建planning group: 这是使用Moveit!的一个核心步骤，需要设置机器人的各个部分的名称及其父级名称。 规划组名称: arm 运动学逆解工具: 初始选择: KDL 后改为: IKFAST 运动规划参数: 可以按照默认值 Kin. Search Resolution: 关节空间的采样密度 Kin. Search Timeout: 求解时间 Kin. Solver Attempts: 求解失败尝试次数 OMPL算法选择RRTConnect 算法，在论文中比较了三种算法，为RRT,RRT*,RRTConnect，可以自己查询资料了解三种算法的区别。 1.5 设置robot poses 这一步可以预先设置好一些机器人姿态，就像宏定义一样，方便后面调用。可以手动调节也可以输入数值来调节，调节的单位为弧度 1.6 配置末端执行器: 这里并没有进行设置。 1.7 设置被动关节: 这里也没有进行设置。 被动关节定义: 被动关节是无法主动运动的关节，也就是说，它依赖于其他部件的运动。设置为被动关节后，该关节不会参与运动规划。 1.8 设置作者等信息: 填写相关的作者、机构、联系方式等信息。 1.9 保存配置文件: 文件名设为 dobot_moveit_config。 "},"gazebo与rviz的联合仿真.html":{"url":"gazebo与rviz的联合仿真.html","title":"第三步","keywords":"","body":"Gazebo 与 RViz 联合仿真 在未配置前，启动以下命令会报错: roslaunch dobot_moveit_config demo_gazebo.launch 错误提示： 这是因为官方提供的urdf少了gazebo少了一些配置，接下来就是为urdf加上相应的配置动态链.so文件。 URDF Gazebo 配置 在URDF文件末端添加配置 transmission_interface/SimpleTransmission hardware_interface/PositionJointInterface hardware_interface/PositionJointInterface 1 transmission_interface/SimpleTransmission hardware_interface/PositionJointInterface hardware_interface/PositionJointInterface 1 transmission_interface/SimpleTransmission hardware_interface/PositionJointInterface hardware_interface/PositionJointInterface 1 transmission_interface/SimpleTransmission hardware_interface/PositionJointInterface hardware_interface/PositionJointInterface 1 / joint_1, joint_2, joint_5,joint_6 解析：这是为关节添加传动机制，硬件接口和执行器，这样它们就可以被控制 而gazebo标签是为urdf添加的gazebo控制器(插件) gazebo_ros_control自身是去参数的，因此会发现启动时会发现获取不了pid参数，其实也可以忽略该错误，也能执行 如果想要pid参数，可以通过ros的ros_control框架和相关的控制器管理器以及机器人模型中的关节控制器来设定pid增益。（也可以直接通过参数服务器来设置增益），并未尝试。。。。 再次启动 roslaunch dobot_moveit_config demo gazebo.launch 会发现成功了，这时在rviz中重新进行运动规划，这时rviz与gazebo同时运动了 "},"Moveit!控制真实机械臂.html":{"url":"Moveit!控制真实机械臂.html","title":"第四步","keywords":"","body":"如何与dobot真实机械臂联合运动 这时候就需要修改相应的文件与添加文件 第一步:将官方提供的ros功能包导入自己的工作空间中，官方功能包中包含dobot机械臂启动的服务文件(就是启动机械臂)，还有一些简单控制机械臂的功能(比如回Home位)等，还有将官方提供的srv文件全部拷贝过来 导入之后，需要修改CMakeLists.txt与package.xml文件，此时比较麻烦需要添加很多东西，比如dobot机械臂srv文件等，这时根据官方提供的CMakeLists.txt与package.xml文件复制粘贴。 然后catkin_make编译成功 此时可以尝试一下启动一下dobot机械臂 roscore rosrun dobot DobotServer ttyUSB0 开启dobot机械臂服务 rosrun dobot DobotClient_Returnzero 回零操作 此时会发现机械臂回零操作功能实现了，说明导入相应包成功了 注：有时候启动服务时，会报错，是因为usb端口权限没开 一次性解决其串口权限 sudo chmod a+rw /dev/ttyUSB0 永久性解决其串口权限 https://blog.csdn.net/c417469898/article/details/117510172 重启电脑才完全生效。。。。。。。。。。。。。。。。。。。。。 第二步：可以查看下面博客，了解下面的修改 https://blog.csdn.net/qq_34935373/article/details/95916111 1:修改dobot_moveit_config/launch包中的文件demo.launch与move_group文件中 然后根据下面图片该注释注释，该添加添加 2: 然后创建 dobot_moveit_controller_manager.launch.xml文件 3.然后在dobot_moveit_config/config中添加controllers.yaml文件 controller_list: - name: arm_controller action_ns: follow_joint_trajectory type: FollowJointTrajectory default: True joints: - joint_1 - joint_2 - joint_5 - joint_6 注 这里controllers.yaml文件不需要自己手写，可以直接simple_moveit_controllers.yaml文件里的内容拷贝出来(这是之前在编写controllers.yaml文件时发现与simple_moveit_controllers.yaml文件一致) 那么这里就有另一种方式修改dobot_moveit_controller_manager.launch.xml文件中最后一行 对应修改为simple_moveit_controllers.yaml。按照习惯的话。还是采用第一种方式吧 4:添加launch文件方便启动各种文件dobot_moveit_planning_executiuon.launch 5：会发现这里并没有dobot_gazebo.launch文件，这里添加该dobot文件中添加launch文件 在 dobot文件夹中 config文件夹添加joint_trajectory_controller.yaml文件，这个文件是创建了一个控制器来控制相关的关节，联合状态发布者joint_state_controller arm_controller: type: \"position_controllers/JointTrajectoryController\" joints: [joint_1,joint_2,joint_5,joint_6] joint_state_controller: type: \"joint_state_controller/JointStateController\" publish_rate: 50 7.先在dobot包里导入JointTrajectory_subscriber.cpp，然后修改cmakelist.txt文件，使其编译成功。 #include \"ros/ros.h\" #include \"std_msgs/String.h\" #include \"std_msgs/Float32MultiArray.h\" #include \"DobotDll.h\" #include #include #include #include #include \"dobot/SetCmdTimeout.h\" #include \"dobot/SetQueuedCmdClear.h\" #include \"dobot/SetQueuedCmdStartExec.h\" #include \"dobot/SetQueuedCmdForceStopExec.h\" #include \"dobot/GetDeviceVersion.h\" #include \"dobot/SetEndEffectorParams.h\" #include \"dobot/SetPTPJointParams.h\" #include \"dobot/SetPTPCoordinateParams.h\" #include \"dobot/SetPTPJumpParams.h\" #include \"dobot/SetPTPCommonParams.h\" #include \"dobot/SetPTPCmd.h\" void callback(const control_msgs::FollowJointTrajectoryActionGoal::ConstPtr &msg); ros::ServiceClient client_1; ros::ServiceClient client_2; ros::ServiceClient client_3; ros::ServiceClient client_4; ros::ServiceClient client_5; ros::ServiceClient client_6; ros::ServiceClient client_7; ros::ServiceClient client_8; ros::ServiceClient client_9; ros::ServiceClient client_10; int main(int argc, char **argv) { ros::init(argc, argv, \"JointTrajectory_subscriber\"); ros::NodeHandle n; ros::ServiceClient client; client_1 = n.serviceClient(\"/DobotServer/SetCmdTimeout\"); client_2 = n.serviceClient(\"/DobotServer/SetQueuedCmdClear\"); client_3 = n.serviceClient(\"/DobotServer/SetQueuedCmdStartExec\"); client_4 = n.serviceClient(\"/DobotServer/GetDeviceVersion\"); client_5 = n.serviceClient(\"/DobotServer/SetEndEffectorParams\"); client_6 = n.serviceClient(\"/DobotServer/SetPTPJointParams\"); client_7 = n.serviceClient(\"/DobotServer/SetPTPCoordinateParams\"); client_8 = n.serviceClient(\"/DobotServer/SetPTPJumpParams\"); client_9 = n.serviceClient(\"/DobotServer/SetPTPCommonParams\"); client_10 = n.serviceClient(\"/DobotServer/SetPTPCmd\"); ROS_INFO(\"JointTrajectory_subscriber running.../n\"); ros::Rate loop_rate(30); ros::Subscriber trajectory_sub = n.subscribe(\"/arm_controller/follow_joint_trajectory/goal\", 10, callback); ros::spin(); return 0; } void callback(const control_msgs::FollowJointTrajectoryActionGoal::ConstPtr &msg) { int trajectory_length = msg->goal.trajectory.points.size(); std::cout goal.trajectory.points[i].positions[0] / (float)M_PI * 180; srv.request.y = (float)msg->goal.trajectory.points[i].positions[1] / (float)M_PI * 180;; srv.request.z = (float)msg->goal.trajectory.points[i].positions[2] / (float)M_PI * 180;; srv.request.r = 0; client_10.call(srv); } */ /*srv.request.ptpMode = 4; srv.request.x = (float)msg->goal.trajectory.points[0].positions[0] / (float)M_PI * 180; srv.request.y = (float)msg->goal.trajectory.points[0].positions[1] / (float)M_PI * 180;; srv.request.z = (float)msg->goal.trajectory.points[0].positions[2] / (float)M_PI * 180;; srv.request.r = 0; client_10.call(srv); */ srv.request.ptpMode = 4; srv.request.x = (double)msg->goal.trajectory.points[trajectory_length-1].positions[0] / (double)M_PI * 180; srv.request.y = (double)msg->goal.trajectory.points[trajectory_length-1].positions[1] / (double)M_PI * 180;; srv.request.z = (double)msg->goal.trajectory.points[trajectory_length-1].positions[2] / (double)M_PI * 180;; srv.request.r = 0; client_10.call(srv); } 最后，可以测试一下是否成功。 1:roscore 2:rosrun dobot DobotServer ttyUSB0 3:roslaunch dobot_moveit_config dobot_moveit_planning_executiuon.launch 4启动JointTrajectory_subscriber.cpp rosrun dobot JointTrajectory_subscriber 此时可以点击rviz中预设的位置，发现真实机械臂与rviz，gazebo中模型同时运动。 补充 effort transmission 力/扭矩传动:该传动类型假定机械臂的控制输入是力/扭矩，用于控制关节的扭矩或力。适用于需要对关节施加特定力或扭矩来实现和力感知的应用 Velocity(速度) 适用于控制执行器以特定的速度运行。它可用于需要执行平滑，连续运动的任务 Position位置 适用于控制执行器移动到特定位置或关节角度。它可用于需要执行准确位置控制的任务。 https://blog.csdn.net/m0_56661101/article/details/131415296 "},"利用c++与python简单控制机械臂.html":{"url":"利用c++与python简单控制机械臂.html","title":"第五步","keywords":"","body":"c++与python简单控制机械臂 Moveit提供的API，这些接口涵盖了运动控制的几乎所有方面，包括动力学，关节控制，规划场景(是表示周围环境状态的抽象模型，它能够监控障碍物的放置和机器人状态，因此可以用于碰撞检测和约束赋值，所有被规划的动作必须指定一个已有的移动组并在指定的规划场景里面进行)和碰撞检测，运动规划和三维感知(使用如激光扫描和点云这类传感器数据来检测物体，并将检测到的物体插入规划场景)。 Moveit提供的API，这些接口涵盖了运动控制的几乎所有方面，包括动力学，关节控制，规划场景(是表示周围环境状态的抽象模型，它能够监控障碍物的放置和机器人状态，因此可以用于碰撞检测和约束赋值，所有被规划的动作必须指定一个已有的移动组并在指定的规划场景里面进行)和碰撞检测，运动规划和三维感知(使用如激光扫描和点云这类传感器数据来检测物体，并将检测到的物体插入规划场景)。 Moveit设置助手 会关注以下几种任务: URDF转换到SRDF：机器人的URDF模型被用于创建增强的机器人语义描述文件(SRDF),SRDF包含多种标签以定义组，末端执行器和自碰撞信息。 附加的配置文件 这些文件定义了机器人的运动学解算器，关节控制器，关节限制，动作规划期和传感器 一系列启动文件:用于运行关键的规划和动作控制模块，包括规划场景流水线，包括碰撞检查和使用octomap的占用网格 使用ompl的动作规划 move_group节点，提供ros主题，服务和各种动作。这些动作用于运动学分析，规划和执行，取放操作，状态校验和规划场景的查询及更新 一个关节轨迹控制器管理器，用于为指定机器人启动合适的轨迹控制器 有两个关键的项目是moveit设置助手不会创建的，就是针对你的机器人特定的关节控制器编写的配置文件和启动文件。 这里并没有提供相关代码，这里可以通过网上查询获取。 "},"cv_bridge的简单使用.html":{"url":"cv_bridge的简单使用.html","title":"第六步","keywords":"","body":"摄像头usb 使用cv_bridge 简单的显示一个红色的圆 #!/usr/bin/env python3 # -*- coding:utf-8 -*- import rospy from cv_bridge import CvBridge, CvBridgeError from sensor_msgs.msg import Image import cv2 class Demo3: def __init__(self): # 创建cv_bridge, 声明图像的发布者和订阅者 self.image_pub = rospy.Publisher(\"cv_image\", Image, queue_size=1) self.bridge = CvBridge() self.image_sub = rospy.Subscriber(\"/usb_cam/image_raw\", Image, self.callback) def callback(self, data): # 使用cv_bridge将ROS的图像数据转换为Opencv的图像格式 try: cv_image = self.bridge.imgmsg_to_cv2(data, 'bgr8') except CvBridgeError as e: print(e) # 在opencv的显示窗口中绘制一个圆，作为标记 (rows, cols, channels) = cv_image.shape if cols > 60 and rows > 60: cv2.circle(cv_image, (60, 60), 30, (0, 0, 255), -1) # 显示Opencv格式的图像 cv2.imshow(\"Image\", cv_image) cv2.waitKey(3) # 再将opencv格式数据转换成ros image格式的数据发布 try: self.image_pub.publish(self.bridge.cv2_to_imgmsg(cv_image, 'bgr8')) except CvBridgeError as e: print(e) if __name__ == '__main__': try: # 初始化ros节点 rospy.init_node(\"cv_bridge_test\") rospy.loginfo(\"开始cv_bridge节点\") Demo3() rospy.spin() except KeyboardInterrupt: print(\"关闭节点\") "},"手势控制机械臂.html":{"url":"手势控制机械臂.html","title":"第七步","keywords":"","body":"使用手势简单的控制机械臂 #!/usr/bin/env python3 #coding=utf-8 import os import sys import time import mediapipe as mp import numpy as np import threading import rospy import cv2 import moveit_commander from geometry_msgs.msg import PoseStamped from sensor_msgs.msg import Image from cv_bridge import CvBridge # Global variables img_path = '/home/root1/dobot_ws/src/dobot/images' img_num = 1 cv_image = np.zeros((480, 640, 3), np.uint8) mp_drawing = mp.solutions.drawing_utils mp_hands = mp.solutions.hands gesture = \"none\" gesture_lock = threading.Lock() bridge = CvBridge() # Initialize MoveIt def MoveitJointpose(): moveit_commander.roscpp_initialize(sys.argv) arm = moveit_commander.MoveGroupCommander('arm') arm.set_goal_joint_tolerance(0.01) arm.set_max_velocity_scaling_factor(0.8) return arm # Display FPS on the image def show_fps(img): global fps fps_text = 'FPS: {:.2f}'.format(fps) cv2.putText(img, fps_text, (10, 20), cv2.FONT_HERSHEY_PLAIN, 1.0, (240, 240, 240), 1, cv2.LINE_AA) return img # Calculate distance between two points def distance(point_1, point_2): return np.sqrt((point_1[0] - point_2[0]) ** 2 + (point_1[1] - point_2[1]) ** 2) # Calculate angle between two vectors def vector_2d_angle(v1, v2): norm_v1_v2 = np.linalg.norm(v1) * np.linalg.norm(v2) cos = v1.dot(v2) / (norm_v1_v2) sin = np.cross(v1, v2) / (norm_v1_v2) angle = np.degrees(np.arctan2(sin, cos)) return angle # Convert hand landmarks to image coordinates def get_hand_landmarks(img_size, landmarks): w, h = img_size landmarks = [(lm.x * w, lm.y * h) for lm in landmarks] return np.array(landmarks) # Calculate angles of each finger def hand_angle(landmarks): angle_list = [] angle_ = vector_2d_angle(landmarks[3] - landmarks[4], landmarks[0] - landmarks[2]) angle_list.append(angle_) angle_ = vector_2d_angle(landmarks[0] - landmarks[6], landmarks[7] - landmarks[8]) angle_list.append(angle_) angle_ = vector_2d_angle(landmarks[0] - landmarks[10], landmarks[11] - landmarks[12]) angle_list.append(angle_) angle_ = vector_2d_angle(landmarks[0] - landmarks[14], landmarks[15] - landmarks[16]) angle_list.append(angle_) angle_ = vector_2d_angle(landmarks[0] - landmarks[18], landmarks[19] - landmarks[20]) angle_list.append(angle_) angle_list = [abs(a) for a in angle_list] return angle_list # Determine gesture based on finger angles def h_gesture(angle_list): # Thresholds for angles thr_angle = 65. thr_angle_thumb = 53. thr_angle_s = 49. gesture_str = \"none\" if (angle_list[0] > thr_angle_thumb) and (angle_list[1] > thr_angle) and (angle_list[2] > thr_angle) and (angle_list[3] > thr_angle) and (angle_list[4] > thr_angle): gesture_str = \"fist\" # Add other gesture conditions here return gesture_str # Image callback function def callback(data): global cv_image try: cv_image = bridge.imgmsg_to_cv2(data, \"bgr8\") except CvBridgeError as e: print(e) # Main function for gesture inference def infer(): global img_num, cv_image, gesture with mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5) as hands: while not rospy.is_shutdown(): image = cv_image.copy() results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) gesture_str = \"none\" if results.multi_hand_landmarks: for hand_landmarks in results.multi_hand_landmarks: mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS) landmarks = get_hand_landmarks((image.shape[1], image.shape[0]), hand_landmarks.landmark) angle_list = hand_angle(landmarks) gesture_str = h_gesture(angle_list) if gesture_str != \"none\": break with gesture_lock: gesture = gesture_str image = show_fps(image) cv2.putText(image, gesture_str, (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 4) cv2.imshow('demo_gesture', image) key_val = cv2.waitKey(33) if key_val == ord('c'): cv2.imwrite(os.path.join(img_path, f'{img_num}.jpg'), image) img_num += 1 # Thread for acting based on detected gesture def infer_act(): global gesture, arm while not rospy.is_shutdown(): with gesture_lock: current_gesture = gesture if current_gesture == \"one\": arm.set_named_target('Home') arm.go() rospy.sleep(1) elif current_gesture == \"two\": arm.set_named_target('eye_hand') arm.go() rospy.sleep(1) time.sleep(0.1) if __name__ == '__main__': rospy.init_node(\"demo_gesture\") arm = MoveitJointpose() rospy.loginfo(\"Starting gesture recognition\") image_sub = rospy.Subscriber(\"/usb_cam/image_raw\", Image, callback) try: thread = threading.Thread(target=infer) thread_act = threading.Thread(target=infer_act) thread.start() thread_act.start() thread.join() thread_act.join() rospy.spin() except KeyboardInterrupt: print(\"Gesture recognition closed\") cv2.destroyAllWindows() "},"内参标定.html":{"url":"内参标定.html","title":"第八步","keywords":"","body":"对摄像头进行内参标定 1.安装robot_vision包 git clone https://mirror.ghproxy.com/https://github.com/1417265678/robot_vision catkin_make 2 安装camera_calibration包 sudo apt-get install ros-noetic-camera-calibration 3 启动标定程序 roslaunch robot_vision usb_cam.launch rosrun camera_calibration cameracalibrator.py --size 8x5 --square 0.027 image:=/usb_cam/image_raw camera:=/usb_cam 这里使用的是内角点为8x5，方块边长为0.027m，标定图像为usb_cam/image_raw，标定相机为usb_cam。 4：标定摄像头 - **把标定板放置在摄像头视野范围内** - **X：标定板在摄像头视野中的左右移动** - **Y：标定板在摄像头视野的上下移动** - **Size：标定板在摄像头视野中的前后移动** - **Skew：标定板在摄像头视野中的倾斜转动** 不断移动标定板，直到CALIBRATE按钮变色，点击开始自动计算摄像头的标定参数，点击save按钮，标定参数将被保存到默认的文件夹下 点击COMMIT按钮，提交数据并退出程序，打开/tmp文件，就可以看到标定结果的压缩文件 解压该文件，找到ost.yaml命名的标定结果文件，将该文件复制出来，重新命名就可以使用了。 内参：需要标定的相机的参数，它决定了物体的实际位置在成像平面上的投影位置 为了检验camera_calibration包获取内参的准确程度，这里导入了另一个包calib_camera包，而获取内参的图片仍然是camera_calibration包获取的图片,发现结果相差不大。 白色图是camera_calibration包获取的，终端是程序获取的。 "},"手眼标定.html":{"url":"手眼标定.html","title":"第九步","keywords":"","body":"手眼标定 下载arous_ros与easy_handeye包git clone https://gitclone.com/github.com/IFL-CAMP/easy_handeye.git git clone https://mirror.ghproxy.com/https://github.com/pal-robotics/aruco_ros.git git clone -b noetic-devel https://github.com/pal-robotics/aruco_ros.git 开始之前，将机械臂与相机的位置进行固定，并打印Marker 这里利用 arous_ros，需要修改single.launch文件，需要将MarkerID，Size等信息进行修改 标定过程 标定结果 接下来就是将手眼标定发布出去 https://www.guyuehome.com/36266 在网上查找出大致两种方式:第一种就是将easy_handeye包里的publish.launch与publish.py进行修改之后，.py文件也需要修改这里并没有提供 如果想要使用第一种方法修改.py文件可以参考王晓云 ros机械臂开发与实践中的代码 另一种就是直接编写launch文件将手眼标定结果发布给tf中静态坐标变换 "},"darknet_ros.html":{"url":"darknet_ros.html","title":"第十步","keywords":"","body":"使用darknrt_ros包简单检测目标 为了避免catkin_make太慢，这里直接将模型文件先下载好 /home/root1/dobot_ws/src/darknet_ros-master/darknet_ros/yolo_network_config/weights文件下,打开文件直接复制粘贴在终端下载即可 进到darknet目录下进行make 此时仍然编译不成功，接下来需要修改文件 (因为自己电脑的GPU与算力太高) 相关参数说明： 修改之后，编译成功，此时还不能运行，因为与自己摄像头的话题不一致，需要修改文件 之后启动 roslaunch usb_cam usb_cam-test.launch roslaunch darknet_ros darknet_ros.launch 此时发现帧率非常高，这是因为在上面配置中使用了GPU，与CUDNN加速 "}}